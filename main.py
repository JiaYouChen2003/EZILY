import time
import random
import argparse
import csv
import torch
import torchvision
import google.generativeai as genai
import tqdm

from fixed_prompts import classification_p, description_p, class_ps
from cross_modal_encoder import encoder
from cifar_100_label import classes_label


def gemini_process(prompt, image=None, temperature=0.99):
    """
    Uses Gemini Pro to generate text based on a prompt, optionally with an image.
    Args:
    prompt: The text prompt for Gemini Pro.
    image_path: Path to the input image (optional).
    temperature: Sampling temperature for generating diverse responses.
    Returns:
    The generated response as a string.
    """
    model = genai.GenerativeModel("models/gemini-1.5-flash")
    
    input_content = [prompt]
    
    if image is not None:
        try:
            input_content.append(image)
        except Exception as e:
            print(f"Error processing image: {e}")
            return None

    response = model.generate_content(
        contents=input_content,
        generation_config={"temperature": temperature, "max_output_tokens": 256}
    )
    
    try:
        # print(f'Prompt: {prompt}\nResponse: {response.candidates[0].content.parts[0].text}')
        # Quota: 15 rpm; this should be fine
        time.sleep(4 + random.random())
        return response.candidates[0].content.parts[0].text
    except Exception as e:
        print(f"Error in response generation: {e}")
        return prompt


def create_classifier(class_names, k=10):
    """
    Constructs a zero-shot image classifier.
    Args:
    class_names: A list of class names.
    class_ps: A list of prompt templates for generating class descriptions.
    k: Number of class descriptions to be generated by the LLM.
    Returns:
    A zero-shot image classification model.
    """
    if k != 0:
        assert k >= len(class_ps), "k should be greater than or equal to the number of class prompts."
        assert k % len(class_ps) == 0, "k should be a multiple of the number of class prompts."

    weights = []
    for class_name in tqdm.tqdm(class_names):
        class_name_feature = encoder.encode_text(class_name)
        template_feature = encoder.encode_text(f"A photo of {class_name}")
        llm_class_description = torch.zeros((1, encoder.output_feature_length))

        if k != 0:
            for _ in range(k // len(class_ps)):
                for class_p in class_ps:
                    llm_description = gemini_process(class_p.format(class_name=class_name), temperature=0.99)
                    llm_class_description += encoder.encode_text(llm_description)

            llm_class_description /= k

        class_feature = class_name_feature + template_feature + llm_class_description
        normalized_class_feature = class_feature / class_feature.norm(dim=-1, keepdim=True)
        weights.append(normalized_class_feature.squeeze())

    weights = torch.stack(weights)
    model = {"weights": weights.T, "class_names": class_names}
    return model


def classify(image, classifier, use_llm=True):
    """
    Performs zero-shot image classification.
    Args:
    image: Input testing image.
    classifier: A zero-shot classification model generated by create_classifier function.
    classification_p: Prompt template for generating the initial classification prediction.
    description_p: Prompt template for generating an image description.
    Returns:
    Predicted class name.
    """
    image_feature = encoder.encode_image(image)

    if use_llm:
        # Gemini Pro for initial classification prediction
        initial_prediction = gemini_process(classification_p.format(classes=classifier["class_names"]), image, temperature=0.99)
        prediction_feature = encoder.encode_text(initial_prediction)

        # Gemini Pro for generating image description
        image_description = gemini_process(description_p, image, temperature=0.99)
        description_feature = encoder.encode_text(image_description)

        query_feature = image_feature + prediction_feature + description_feature
    else:
        query_feature = image_feature

    query_feature /= query_feature.norm(dim=-1, keepdim=True)

    logits = torch.matmul(query_feature, classifier["weights"])
    index = torch.argmax(logits, dim=-1)
    return classifier["class_names"][index.item()]


def main(use_llm, k=5):
    testset = torchvision.datasets.CIFAR100(
        root='./data',
        train=False,
        download=True,
    )

    if use_llm:
        suffix = 'basic'
        print('USING_LLM')
        with open('key2.txt', "r") as file:
            api_key = file.read().strip()
        genai.configure(api_key=api_key)
        classifier = create_classifier(class_names=classes_label, k=k)
        torch.save(classifier, f"zero_shot_classifier_{suffix}.pth")
    else:
        suffix = 'without_llm'
        print('NOT_USING_LLM')
        classifier = create_classifier(class_names=classes_label, k=0)
        torch.save(classifier, f"zero_shot_classifier_{suffix}.pth")

    correct_count = 0
    mistake_count = 0
    with open(f'mistake_predicted_result_{suffix}.csv', "w", newline='') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(['Predict', 'Label'])
        for i, (img, label) in enumerate(tqdm.tqdm(testset, total=200)):
            if i >= 200:
                break
            predicted_label = classify(img, classifier, use_llm=use_llm)
            if classes_label[label] == predicted_label:
                correct_count += 1
                print(f'Correct! Predicted: {predicted_label}; Label: {classes_label[label]}; Count: {correct_count}')
            else:
                mistake_count += 1
                writer.writerow([predicted_label, classes_label[label]])
                print(f'Mistake. Predicted: {predicted_label}; Label: {classes_label[label]}; Amount: {mistake_count}')
    print(f'Accuracy: {(100 * correct_count / (correct_count + mistake_count)):.4f}%')


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument("--use_llm", action="store_true", help="Set to enable LLM usage")
    args = parser.parse_args()

    main(args.use_llm)
